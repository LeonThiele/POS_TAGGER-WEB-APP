{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import keras\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.models import load_model\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3131\n",
      "783\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "\n",
    "#split the data into train/test\n",
    "train, test = sklearn.model_selection.train_test_split(tagged_sentences, test_size=0.2, random_state=1);\n",
    "\n",
    "#removes -NONE- tags\n",
    "def cleanup(dat):\n",
    "    l =[]\n",
    "    \n",
    "    for di in dat:\n",
    "        l2= []\n",
    "        count += 1\n",
    "        for x,y in di:\n",
    "            \n",
    "            if y == '-NONE-':\n",
    "                continue\n",
    "            if y == 'SYM':\n",
    "                continue\n",
    "            \n",
    "            l2.append((x,y))\n",
    "        l.append(l2)\n",
    "    \n",
    "    return l\n",
    "        \n",
    "train = cleanup(train)\n",
    "test = cleanup(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splits the data into X and y. Where X contains the sentences and y the corresponding tags\n",
    "def transform(sentence):\n",
    "    \n",
    "    X, y = [],[];\n",
    "    for x in sentence:\n",
    "        for i in range(len(x)):\n",
    "            \n",
    "            X.append(x[i][0]);\n",
    "            y.append(x[i][1]);\n",
    "\n",
    "\n",
    "    return X, y;\n",
    "\n",
    "#creates a dictionarie: number to tag/word\n",
    "def tokenize(data_text):\n",
    "    \n",
    "    dic = dict()\n",
    "    i = 0\n",
    "    for x in data_text:\n",
    "        if x in dic:\n",
    "            continue\n",
    "        dic[x] = i\n",
    "        i+=1\n",
    "\n",
    "    return dic\n",
    "\n",
    "\n",
    "def to_hot(dat, dic):\n",
    "    l=[]\n",
    "    \n",
    "    for tu in dat:\n",
    "        for x,y in tu:\n",
    "           \n",
    "            l.append([dic[x]])\n",
    "    \n",
    "    return l\n",
    "\n",
    "def myEncoder(ar,dic):\n",
    "    l= []\n",
    "    for y in ar:\n",
    "        l.append(dic[y])\n",
    "        \n",
    "    return l\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into x and y\n",
    "X_train, y_train = transform(train)\n",
    "X_test, y_test = transform(test)\n",
    "\n",
    "\n",
    "\n",
    "word_index = tokenize(X_train + X_test)\n",
    "tag_index = tokenize(y_train + y_test)\n",
    "number_to_word = {v: k for k, v in word_index.items()}\n",
    "number_to_tag = {v: k for k, v in tag_index.items()}\n",
    "\n",
    "\n",
    "#encode X and y data\n",
    "y_train = myEncoder(y_train, tag_index)\n",
    "y_test = myEncoder(y_test, tag_index)\n",
    "    \n",
    "X_train = to_hot(train, word_index)\n",
    "X_test = to_hot(test, word_index)\n",
    "\n",
    "\n",
    "onehotencoder = OneHotEncoder(handle_unknown='ignore')\n",
    "onehotencoder.fit(X_train + X_test)\n",
    "\n",
    "\n",
    "X_train = onehotencoder.transform(X_train).toarray()\n",
    "X_test = onehotencoder.transform(X_test).toarray()\n",
    "\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60281 samples, validate on 15071 samples\n",
      "Epoch 1/2\n",
      "60281/60281 [==============================] - 149s 2ms/step - loss: 1.0966 - acc: 0.6884 - val_loss: 0.4312 - val_acc: 0.8770\n",
      "Epoch 2/2\n",
      "60281/60281 [==============================] - 143s 2ms/step - loss: 0.2479 - acc: 0.9328 - val_loss: 0.3721 - val_acc: 0.8853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d570820860>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def model(input_dim, hidden_neurons, output_dim):\n",
    "    model = models.Sequential([\n",
    "        Dense(hidden_neurons, input_dim=input_dim, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(hidden_neurons, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_dim, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "mod = model(X_train.shape[1], 128, y_train.shape[1])\n",
    "\n",
    "\n",
    "mod.fit(X_train, y_train, epochs=2, validation_split =0.2, batch_size = 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18731/18731 [==============================] - 7s 392us/step\n",
      "The score is: 0.8791842400267148\n"
     ]
    }
   ],
   "source": [
    "score = mod.evaluate(X_test, y_test)\n",
    "print(\"The score is:\",score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod.save('Hot_NLP_model.h5')\n",
    "#mod = load_model('Hot_NLP_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate(words, dic):\n",
    "    sent = words.split()\n",
    "    l = []\n",
    "    for x in sent:\n",
    "        l.append([dic[x]])\n",
    "\n",
    "    p = onehotencoder.transform(l).toarray()\n",
    "    \n",
    "    encoded_tags = mod.predict_classes(p)\n",
    "    \n",
    "    print(encoded_tags)\n",
    "    \n",
    "    l=[]\n",
    "    for e in encoded_tags:\n",
    "        l.append(number_to_tag[e])\n",
    "            \n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27  7 12 11  6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['WDT', 'CD', 'MD', 'PRP', 'IN']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"which one would you like\", word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
